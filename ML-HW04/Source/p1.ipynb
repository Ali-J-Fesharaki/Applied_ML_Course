{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "driver.get('https://www.lusha.com/company-search/software-development/021d3a8f4f/iran-islamic-republic-of/b50af49bc6/')\n",
    "\n",
    "# Extract English company names\n",
    "english_company_names = []\n",
    "for i in range(0, 50):\n",
    "    try:\n",
    "        company_css_selector = f'section.directory-content:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child({(i//3)+1}) > div:nth-child({(i%3)+1}) > a:nth-child(1)'\n",
    "        company_element =driver.find_element(By.CSS_SELECTOR,company_css_selector)\n",
    "        print(i)\n",
    "        english_company_names.append(company_element.text)\n",
    "    except:\n",
    "        print(\"error\",i)\n",
    "        pass\n",
    "\n",
    "# Clean up\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'English Company Name': english_company_names})\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel('software_engineering_companies.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0&amp;1 Information Technology Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2A Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7030.ir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACACO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AKAF SYSTEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARIISCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ARYANIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abarsazeha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adak Software Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aien Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AkafWeb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alo Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amin Design Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Amirkabir Data Miners | داده کاوان امیرکبیر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anasys Softs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Andishe Computer (اندیشه کامپیوتر)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AppTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Arabit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ariana Labs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ariasun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arin Corporations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Arioobarzan Team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Arka Data Processing Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Arma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Arosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Arsina software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Artin system CO . آرتین سیستم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Asa Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Atency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Atitel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Avihang Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BARG System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Bahar Aram Data Process Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bahasaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Balad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Bamdad Knowledge and Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Baridsoft | برید سامانه نوین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Behine Sazan Farayand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Behineh Iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Behineh Pardaz Tarnian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Behpardaz Jahan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Behpooyesh | بهپویش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Behsaa BPMS + ERP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>BisPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bit&amp;-شرکت فرآیندهای پویای کسب و کار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>BlueBitSoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Boomrang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Borhan System Pasargad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>BornaRayaneh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>CATS EYE SMART SYSTEMS LTD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           English Company Name\n",
       "0                0&1 Information Technology Co.\n",
       "1                                    2A Company\n",
       "2                                       7030.ir\n",
       "3                                         ACACO\n",
       "4                                   AKAF SYSTEM\n",
       "5                                       ARIISCO\n",
       "6                                       ARYANIC\n",
       "7                                    Abarsazeha\n",
       "8                         Adak Software Company\n",
       "9                                       Aien Co\n",
       "10                                      AkafWeb\n",
       "11                              Alo Application\n",
       "12                            Amin Design Group\n",
       "13  Amirkabir Data Miners | داده کاوان امیرکبیر\n",
       "14                                 Anasys Softs\n",
       "15           Andishe Computer (اندیشه کامپیوتر)\n",
       "16                                      AppTech\n",
       "17                                       Arabit\n",
       "18                                  Ariana Labs\n",
       "19                                      Ariasun\n",
       "20                            Arin Corporations\n",
       "21                             Arioobarzan Team\n",
       "22                Arka Data Processing Approach\n",
       "23                                         Arma\n",
       "24                                       Arosis\n",
       "25                              Arsina software\n",
       "26                Artin system CO . آرتین سیستم\n",
       "27                                      Asa Co.\n",
       "28                                       Atency\n",
       "29                                       Atitel\n",
       "30                              Avihang Company\n",
       "31                                  BARG System\n",
       "32                  Bahar Aram Data Process Co.\n",
       "33                                      Bahasaz\n",
       "34                                        Balad\n",
       "35              Bamdad Knowledge and Technology\n",
       "36                 Baridsoft | برید سامانه نوین\n",
       "37                        Behine Sazan Farayand\n",
       "38                                 Behineh Iran\n",
       "39                       Behineh Pardaz Tarnian\n",
       "40                              Behpardaz Jahan\n",
       "41                          Behpooyesh | بهپویش\n",
       "42                            Behsaa BPMS + ERP\n",
       "43                                     BisPhone\n",
       "44          Bit&-شرکت فرآیندهای پویای کسب و کار\n",
       "45                                  BlueBitSoft\n",
       "46                                     Boomrang\n",
       "47                       Borhan System Pasargad\n",
       "48                                 BornaRayaneh\n",
       "49                  CATS EYE SMART SYSTEMS LTD."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load company names from Excel file\n",
    "companies_df = pd.read_excel('software_engineering_companies.xlsx')\n",
    "companies = companies_df['English Company Name'].tolist()\n",
    "\n",
    "# Set up Selenium WebDriver for Firefox\n",
    "geckodriver_path = '/path/to/geckodriver'\n",
    "driver = webdriver.Firefox(executable_path=geckodriver_path)\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "# Log in to LinkedIn (manual interaction required)\n",
    "input(\"Press Enter after logging in...\")\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "for company in companies:\n",
    "    search_url = f\"https://www.linkedin.com/search/results/people/?currentCompany=%5B{company}%5D&keywords=software%20engineer&origin=GLOBAL_SEARCH_HEADER\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    profiles = soup.select('.search-result__info')\n",
    "\n",
    "    for profile in profiles:\n",
    "        try:\n",
    "            full_name = profile.find('span', class_='actor-name').text.strip()\n",
    "            headline = profile.find('p', class_='subline-level-1').text.strip()\n",
    "            location = profile.find('p', class_='subline-level-2').text.strip()\n",
    "\n",
    "            # Click on the profile link to get more details\n",
    "            profile_link = profile.find('a')['href']\n",
    "            driver.get(f\"https://www.linkedin.com{profile_link}\")\n",
    "            time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "            profile_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            experiences = profile_soup.select('.pv-entity__summary-info')\n",
    "            work_experiences = [exp.find('h3').text.strip() for exp in experiences[:3]]\n",
    "            education_section = profile_soup.select('.pv-education-entity')\n",
    "            education_qualifications = [edu.find('h3').text.strip() for edu in education_section[:3]]\n",
    "            skills = profile_soup.select('.pv-skill-category-entity__name-text')\n",
    "            key_skills = [skill.text.strip() for skill in skills[:5]]\n",
    "\n",
    "            # Collect the data\n",
    "            data.append({\n",
    "                'Full name': full_name,\n",
    "                'Email': None,  # LinkedIn usually does not provide emails\n",
    "                'Website': None,  # Custom scraping logic needed for personal websites\n",
    "                'LinkedIn headline': headline,\n",
    "                'City and country of residence': location,\n",
    "                'Current company': company,\n",
    "                'Three work experiences': work_experiences,\n",
    "                'Three educational qualifications': education_qualifications,\n",
    "                'Five key skills': key_skills\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Clean up\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel('software_engineering_profiles.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "\n",
    "class LinkedInProfileSpider(scrapy.Spider):\n",
    "    name = 'linkedin_profiles'\n",
    "    start_urls = ['https://www.linkedin.com/jobs/search?keywords=Software%20Engineer&location=Iran']\n",
    "\n",
    "    def parse(self, response):\n",
    "        profiles = response.css('.result-card__full-card-link::attr(href)').getall()\n",
    "        for profile in profiles:\n",
    "            yield scrapy.Request(profile, callback=self.parse_profile)\n",
    "\n",
    "    def parse_profile(self, response):\n",
    "        full_name = response.css('.top-card-layout__title::text').get().strip()\n",
    "        headline = response.css('.top-card-layout__headline::text').get().strip()\n",
    "        location = response.css('.top-card-layout__first-subline::text').get().strip()\n",
    "\n",
    "        work_experiences = response.css('.experience-item__title::text').getall()[:3]\n",
    "        education_qualifications = response.css('.education__item__degree-info::text').getall()[:3]\n",
    "        key_skills = response.css('.skill::text').getall()[:5]\n",
    "\n",
    "        yield {\n",
    "            'Full name': full_name,\n",
    "            'Email': None,\n",
    "            'Website': None,\n",
    "            'LinkedIn headline': headline,\n",
    "            'City and country of residence': location,\n",
    "            'Current company': None,\n",
    "            'Three work experiences': work_experiences,\n",
    "            'Three educational qualifications': education_qualifications,\n",
    "            'Five key skills': key_skills\n",
    "        }\n",
    "\n",
    "# Run Scrapy spider\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'linkedin_profiles.json'\n",
    "})\n",
    "process.crawl(LinkedInProfileSpider)\n",
    "process.start()\n",
    "\n",
    "# Load JSON and convert to DataFrame\n",
    "profiles_df = pd.read_json('linkedin_profiles.json')\n",
    "profiles_df.to_excel('software_engineering_profiles_scrapy.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Load company names from Excel file\n",
    "companies_df = pd.read_excel('software_engineering_companies.xlsx')\n",
    "companies = companies_df['English Company Name'].tolist()\n",
    "\n",
    "# Define API key and endpoint\n",
    "api_key = 'your_fake_api_key'\n",
    "api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "}\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "for company in companies:\n",
    "    params = {\n",
    "        'url': 'https://www.linkedin.com/in/amir-fesharaki-244a71290/',\n",
    "        'use_cache': 'if-present',\n",
    "    }\n",
    "    response = requests.get(api_endpoint, headers=headers, params=params)\n",
    "    profile_data = response.json()\n",
    "\n",
    "    if response.status_code == 200 and 'data' in profile_data:\n",
    "        profile = profile_data['data']\n",
    "        data.append({\n",
    "            'Full name': profile.get('full_name'),\n",
    "            'Email': profile.get('email'),\n",
    "            'Website': profile.get('website'),\n",
    "            'LinkedIn headline': profile.get('headline'),\n",
    "            'City and country of residence': profile.get('location'),\n",
    "            'Current company': profile.get('current_company'),\n",
    "            'Three work experiences': profile.get('experiences', [])[:3],\n",
    "            'Three educational qualifications': profile.get('education', [])[:3],\n",
    "            'Five key skills': profile.get('skills', [])[:5]\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel('software_engineering_profiles_proxycurl.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Load company names from Excel file\n",
    "companies_df = pd.read_excel('software_engineering_companies.xlsx')\n",
    "companies = companies_df['English Company Name'].tolist()\n",
    "\n",
    "# Define API key and endpoint\n",
    "api_key = 'your_fake_api_key'\n",
    "api_endpoint = 'https://api.example.com/profiles'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "}\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "for company in companies:\n",
    "    params = {\n",
    "        'company': company,\n",
    "        'keywords': 'software engineer',\n",
    "        'location': 'Iran'\n",
    "    }\n",
    "    response = requests.get(api_endpoint, headers=headers, params=params)\n",
    "    profiles_data = response.json()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        for profile in profiles_data['profiles']:\n",
    "            data.append({\n",
    "                'Full name': profile.get('full_name'),\n",
    "                'Email': profile.get('email'),\n",
    "                'Website': profile.get('website'),\n",
    "                'LinkedIn headline': profile.get('headline'),\n",
    "                'City and country of residence': profile.get('location'),\n",
    "                'Current company': company,\n",
    "                'Three work experiences': profile.get('experiences', [])[:3],\n",
    "                'Three educational qualifications': profile.get('education', [])[:3],\n",
    "                'Five key skills': profile.get('skills', [])[:5]\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel('software_engineering_profiles_api.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM SCRAPING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapegraphai import ScrapeGraph\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize ScrapeGraph with a generated API key\n",
    "api_key = '123e4567-e89b-12d3-a456-426614174000'\n",
    "scrapegraph = ScrapeGraph(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load company names from Excel file\n",
    "companies_df = pd.read_excel('software_engineering_companies.xlsx')\n",
    "companies = companies_df['English Company Name'].tolist()\n",
    "\n",
    "# Define the scraping prompt for ScrapeGraphAI\n",
    "prompt = \"\"\"\n",
    "Create a web scraper to collect data on active individuals in the \"Software Engineering\" field in Iran. \n",
    "These individuals should currently work at one of the top 50 domestic companies.\n",
    "The dataset should include at least the following features for each person:\n",
    "- Full name\n",
    "- Email (if available)\n",
    "- Website (if available)\n",
    "- LinkedIn headline\n",
    "- City and country of residence\n",
    "- Current company\n",
    "- Three work experiences\n",
    "- Three educational qualifications\n",
    "- Five key skills\n",
    "Aim for a dataset with at least 2000 records.\n",
    "\"\"\"\n",
    "\n",
    "# Function to collect data for each company\n",
    "def collect_data_for_company(company_name):\n",
    "    query = {\n",
    "        \"company\": company_name,\n",
    "        \"keywords\": \"Software Engineer\",\n",
    "        \"location\": \"Iran\"\n",
    "    }\n",
    "    response = scrapegraph.run(prompt, query)\n",
    "    return response\n",
    "\n",
    "# Create a list to store the collected data\n",
    "data = []\n",
    "\n",
    "# Collect data for each company\n",
    "for company in companies:\n",
    "    try:\n",
    "        company_data = collect_data_for_company(company)\n",
    "        for profile in company_data:\n",
    "            data.append({\n",
    "                'Full name': profile.get('full_name'),\n",
    "                'Email': profile.get('email'),\n",
    "                'Website': profile.get('website'),\n",
    "                'LinkedIn headline': profile.get('headline'),\n",
    "                'City and country of residence': profile.get('location'),\n",
    "                'Current company': company,\n",
    "                'Three work experiences': profile.get('experiences', [])[:3],\n",
    "                'Three educational qualifications': profile.get('education', [])[:3],\n",
    "                'Five key skills': profile.get('skills', [])[:5]\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {company}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to Excel file\n",
    "df.to_excel('software_engineering_profiles_scrapegraphai.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generation prompt for ScrapeGraphAI\n",
    "generation_prompt = \"\"\"\n",
    "Generate a dataset with a minimum of 100 records based on the following input criteria:\n",
    "- Field of activity\n",
    "- Country\n",
    "- University\n",
    "- Current company\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate dataset based on input criteria\n",
    "def generate_dataset(field_of_activity, country, university, current_company):\n",
    "    query = {\n",
    "        \"field_of_activity\": field_of_activity,\n",
    "        \"country\": country,\n",
    "        \"university\": university,\n",
    "        \"current_company\": current_company\n",
    "    }\n",
    "    response = scrapegraph.run(generation_prompt, query)\n",
    "    return response\n",
    "\n",
    "# Example input criteria\n",
    "field_of_activity = \"Software Engineering\"\n",
    "country = \"Iran\"\n",
    "university = \"University of Tehran\"\n",
    "current_company = \"Snapp\"\n",
    "\n",
    "# Generate dataset\n",
    "dataset = generate_dataset(field_of_activity, country, university, current_company)\n",
    "\n",
    "# Create DataFrame\n",
    "df_generated = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to Excel file\n",
    "df_generated.to_excel('generated_dataset_scrapegraphai.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generation prompt for ScrapeGraphAI\n",
    "generation_prompt = \"\"\"\n",
    "Generate a dataset with a minimum of 100 records based on the following input criteria:\n",
    "- Field of activity\n",
    "- Country\n",
    "- University\n",
    "- Current company\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate dataset based on input criteria\n",
    "def generate_dataset(field_of_activity, country, university, current_company):\n",
    "    query = {\n",
    "        \"field_of_activity\": field_of_activity,\n",
    "        \"country\": country,\n",
    "        \"university\": university,\n",
    "        \"current_company\": current_company\n",
    "    }\n",
    "    response = scrapegraph.run(generation_prompt, query)\n",
    "    return response\n",
    "\n",
    "# Example input criteria\n",
    "field_of_activity = \"Software Engineering\"\n",
    "country = \"Iran\"\n",
    "university = \"University of Tehran\"\n",
    "current_company = \"Snapp\"\n",
    "\n",
    "# Generate dataset\n",
    "dataset = generate_dataset(field_of_activity, country, university, current_company)\n",
    "\n",
    "# Create DataFrame\n",
    "df_generated = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to Excel file\n",
    "df_generated.to_excel('generated_dataset_scrapegraphai.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
